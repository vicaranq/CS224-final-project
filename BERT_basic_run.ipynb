{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "745e8cd3-3e98-488f-bf74-b18dcb70c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "import dataset\n",
    "import vsm\n",
    "import sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78053f0e-3c76-4411-a75e-6b54cb01fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER = 2\n",
    "TWITTER_AIRLINES = 3\n",
    "TWITTER_APPLE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f3c90b-58d5-41ba-85ad-f6261f6e20df",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_train, twitter_validate, twitter_test =  dataset.dataset_reader(TWITTER)\n",
    "[twitter_train, twitter_validate, twitter_test] = list(map(lambda ds : dataset.prune_columns(2, ds), [twitter_train, twitter_validate, twitter_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68bd91d8-7dd3-430d-95ec-245c53bcfbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values of sentiment\n",
    "twitter_sentiment_labels = twitter_train['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c570e3-eaa7-43cd-96e2-097988e1a32d",
   "metadata": {},
   "source": [
    "## Pre-trained BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79f193eb-13e6-41ab-bb47-7ed916bef9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_weights_name = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_weights_name)\n",
    "model = BertModel.from_pretrained(bert_weights_name)\n",
    "# model = BertForSequenceClassification.from_pretrained(bert_weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "851b8461-1442-4a1f-9d62-d02cff896093",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt = 'When was I last outside? I am stuck at home for 2 weeks.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c0040ed-a955-4957-a1a7-8208168b55ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee742988-900c-45c4-86bc-d07b7da2699c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sentence: When was I last outside? I am stuck at home for 2 weeks.\n",
      "   Tokens: ['When', 'was', 'I', 'last', 'outside', '?', 'I', 'am', 'stuck', 'at', 'home', 'for', '2', 'weeks', '.']\n",
      "Token IDs: [1332, 1108, 146, 1314, 1796, 136, 146, 1821, 5342, 1120, 1313, 1111, 123, 2277, 119]\n"
     ]
    }
   ],
   "source": [
    "print(f' Sentence: {sample_txt}')\n",
    "print(f'   Tokens: {tokens}')\n",
    "print(f'Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b12d2b1-2f0e-4d88-b95a-eb8e2a5729b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/echyam/miniconda3/envs/nlu/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2149: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "  sample_txt,\n",
    "  max_length=32,\n",
    "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "  return_token_type_ids=False,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',  # Return PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a25d7f9-c9f5-457a-b1c5-46c29c7e7ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1332, 1108,  146, 1314, 1796,  136,  146, 1821, 5342, 1120, 1313,\n",
       "         1111,  123, 2277,  119,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45c48554-bf9b-46b7-9220-f0acbb2f09c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4c02746-346c-4ead-ae30-b660599af0d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'When',\n",
       " 'was',\n",
       " 'I',\n",
       " 'last',\n",
       " 'outside',\n",
       " '?',\n",
       " 'I',\n",
       " 'am',\n",
       " 'stuck',\n",
       " 'at',\n",
       " 'home',\n",
       " 'for',\n",
       " '2',\n",
       " 'weeks',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "511bc927-4576-4cf8-9707-d11e772801c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_num(label):\n",
    "    if label == \"Positive\":\n",
    "        return 1\n",
    "    if label == \"Neutral\":\n",
    "        return 2\n",
    "    if label == \"Negative\":\n",
    "        return 3\n",
    "    if label == \"Irrelevant\":\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed83c6-62d5-4282-9155-8865244fa434",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e1ccfad-fc7a-482f-88dd-ef928386650a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative      314\n",
       "Positive      287\n",
       "Neutral       227\n",
       "Irrelevant    172\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch1 = twitter_train[:1000]\n",
    "batch1.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d30642f-cf32-4db6-bbd8-47156a53662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "for txt in batch1.text:\n",
    "  tokens = tokenizer.encode(str(txt), max_length=512)\n",
    "  token_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74941d16-8c54-41ac-8b86-eff83ed3567c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(token_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c080e4a6-ac64-4229-9a01-e7aa15ba1ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5434743d-9da1-4c5a-96d5-2af552f810c5",
   "metadata": {},
   "source": [
    "# Transform input to feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702be3d2-f410-4e33-8797-d1f0831aa64e",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a026d19-e95b-489c-9b08-ecd6dc274cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = batch1.text.apply(lambda x: tokenizer.encode(str(x), add_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a839ffef-6945-4d75-b126-5089a19b1517",
   "metadata": {},
   "source": [
    "## Pad for matrix ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebfd1d83-8444-4bb3-9244-eb24876e2776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 176)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3937af55-e16f-45ef-b0ef-8d8082c91cb0",
   "metadata": {},
   "source": [
    "## Mask padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f98c61a-ebfb-4d3f-909e-9591bfbad5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 176)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1adc1790-0449-4339-a62c-7e2eb909019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182cca46-fd58-4e97-8f2a-d6dc1072fbfa",
   "metadata": {},
   "source": [
    "## Run Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2026e18f-54d2-40cc-b6d8-183574f9dafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 38s, sys: 1min 31s, total: 14min 10s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d169866-3b02-47a3-ad85-edb4d48b4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state, pooled_output = output.last_hidden_state, output.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83589610-1a1b-4f89-9f49-fe4851603760",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c718a3-a49f-4452-ac07-e4060a0ca24f",
   "metadata": {},
   "source": [
    "## Feature Matrix Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "980d12d1-4b02-4b13-8c04-2f36f932e7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.7202411e-01,  1.2907133e-01,  4.1557610e-02, ...,\n",
       "        -3.1512013e-01,  5.0587171e-01,  2.4475679e-01],\n",
       "       [ 1.7508554e-01,  3.2480410e-01, -1.2354153e-01, ...,\n",
       "        -1.5554804e-01,  2.7715233e-01, -4.4363603e-02],\n",
       "       [ 3.9038855e-01,  2.0172113e-01,  3.7404778e-04, ...,\n",
       "        -1.3712682e-01,  1.7696221e-01,  6.4126499e-02],\n",
       "       ...,\n",
       "       [ 4.5027325e-01,  3.6462277e-01,  3.2491732e-01, ...,\n",
       "        -1.6248864e-01,  3.1678092e-01, -2.2206199e-01],\n",
       "       [ 5.1715469e-01,  2.0905435e-02, -5.8483168e-02, ...,\n",
       "        -3.0024618e-01,  2.7116129e-01, -6.5983713e-02],\n",
       "       [ 4.5719910e-01,  3.0957070e-01, -2.1945217e-01, ...,\n",
       "        -2.2497639e-01,  5.7954282e-01,  8.1879333e-02]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "313aee78-54f1-4937-aa27-1ced34ee6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = batch1.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e8f743-ceab-449c-9ee7-200a1785cb16",
   "metadata": {},
   "source": [
    "# Use BERT Representations with LogisticRegression Softmax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30dae3b5-f005-418c-8e46-9bd8758cf351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "import dataset\n",
    "import vsm\n",
    "import sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c995c70-1ec0-47c7-be85-5a622874c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER = 2\n",
    "TWITTER_AIRLINES = 3\n",
    "TWITTER_APPLE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5059a022-607d-493b-8e7e-82937409db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_train, twitter_validate, twitter_test =  dataset.dataset_reader(TWITTER)\n",
    "[twitter_train, twitter_validate, twitter_test] = list(map(lambda ds : dataset.prune_columns(2, ds), [twitter_train, twitter_validate, twitter_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b41be400-82cf-469e-96b0-522cf9de12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_weights_name = 'bert-base-cased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_weights_name)\n",
    "bert_model = BertModel.from_pretrained(bert_weights_name)\n",
    "# model = BertForSequenceClassification.from_pretrained(bert_weights_name)\n",
    "# Unique values of sentiment\n",
    "twitter_sentiment_labels = twitter_train['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23b9013-5def-4e2a-a699-3bb3b3cd984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_softmax_classifier(X, y):\n",
    "    mod = LogisticRegression(\n",
    "        fit_intercept=True,\n",
    "        solver='liblinear',\n",
    "        multi_class='ovr')\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "466a1797-1ce2-4a94-b900-776777db0d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_cls_phi(text):\n",
    "    # Get the ids. `vsm.hf_encode` will help; be sure to\n",
    "    # set `add_special_tokens=True`.\n",
    "    ##### YOUR CODE HERE\n",
    "    subtok_ids = vsm.hf_encode(text, bert_tokenizer, add_special_tokens=True)\n",
    "\n",
    "    # Get the BERT representations. `vsm.hf_represent` will help:\n",
    "    ##### YOUR CODE HERE\n",
    "    subtok_reps = vsm.hf_represent(subtok_ids, bert_model, layer=-1)\n",
    "\n",
    "    # Index into `reps` to get the representation above [CLS].\n",
    "    # The shape of `reps` should be (1, n, 768), where n is the\n",
    "    # number of tokens. You need the 0th element of the 2nd dim:\n",
    "    ##### YOUR CODE HERE\n",
    "    cls_rep = subtok_reps[0][:][0]\n",
    "\n",
    "    # These conversions should ensure that you can work with the\n",
    "    # representations flexibly. Feel free to change the variable\n",
    "    # name:\n",
    "    return cls_rep.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c69f6-6084-4316-899d-b2ec9d52d16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bert_experiment = sst.experiment(\n",
    "    twitter_train,\n",
    "    hf_cls_phi,\n",
    "    fit_softmax_classifier,\n",
    "    assess_dataframes=[twitter_validate],\n",
    "    vectorize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb900e-ec8f-4e7c-92e2-987c363777e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

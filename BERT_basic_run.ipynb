{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e8cd3-3e98-488f-bf74-b18dcb70c0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "import dataset\n",
    "import vsm\n",
    "import sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78053f0e-3c76-4411-a75e-6b54cb01fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER = 2\n",
    "TWITTER_AIRLINES = 3\n",
    "TWITTER_APPLE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3c90b-58d5-41ba-85ad-f6261f6e20df",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_train, twitter_validate, twitter_test =  dataset.dataset_reader(TWITTER)\n",
    "[twitter_train, twitter_validate, twitter_test] = list(map(lambda ds : dataset.prune_columns(2, ds), [twitter_train, twitter_validate, twitter_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd91d8-7dd3-430d-95ec-245c53bcfbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values of sentiment\n",
    "twitter_sentiment_labels = twitter_train['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c570e3-eaa7-43cd-96e2-097988e1a32d",
   "metadata": {},
   "source": [
    "## Pre-trained BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f193eb-13e6-41ab-bb47-7ed916bef9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_weights_name = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_weights_name)\n",
    "model = BertModel.from_pretrained(bert_weights_name)\n",
    "# model = BertForSequenceClassification.from_pretrained(bert_weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851b8461-1442-4a1f-9d62-d02cff896093",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_txt = 'When was I last outside? I am stuck at home for 2 weeks.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0040ed-a955-4957-a1a7-8208168b55ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee742988-900c-45c4-86bc-d07b7da2699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f' Sentence: {sample_txt}')\n",
    "print(f'   Tokens: {tokens}')\n",
    "print(f'Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b12d2b1-2f0e-4d88-b95a-eb8e2a5729b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "  sample_txt,\n",
    "  max_length=32,\n",
    "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "  return_token_type_ids=False,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',  # Return PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25d7f9-c9f5-457a-b1c5-46c29c7e7ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c48554-bf9b-46b7-9220-f0acbb2f09c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c02746-346c-4ead-ae30-b660599af0d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511bc927-4576-4cf8-9707-d11e772801c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_num(label):\n",
    "    if label == \"Positive\":\n",
    "        return 1\n",
    "    if label == \"Neutral\":\n",
    "        return 2\n",
    "    if label == \"Negative\":\n",
    "        return 3\n",
    "    if label == \"Irrelevant\":\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed83c6-62d5-4282-9155-8865244fa434",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ccfad-fc7a-482f-88dd-ef928386650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = twitter_train[:1000]\n",
    "batch1.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d30642f-cf32-4db6-bbd8-47156a53662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lens = []\n",
    "for txt in batch1.text:\n",
    "  tokens = tokenizer.encode(str(txt), max_length=512)\n",
    "  token_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74941d16-8c54-41ac-8b86-eff83ed3567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(token_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c080e4a6-ac64-4229-9a01-e7aa15ba1ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5434743d-9da1-4c5a-96d5-2af552f810c5",
   "metadata": {},
   "source": [
    "# Transform input to feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702be3d2-f410-4e33-8797-d1f0831aa64e",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a026d19-e95b-489c-9b08-ecd6dc274cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = batch1.text.apply(lambda x: tokenizer.encode(str(x), add_special_tokens = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a839ffef-6945-4d75-b126-5089a19b1517",
   "metadata": {},
   "source": [
    "## Pad for matrix ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd1d83-8444-4bb3-9244-eb24876e2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3937af55-e16f-45ef-b0ef-8d8082c91cb0",
   "metadata": {},
   "source": [
    "## Mask padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f98c61a-ebfb-4d3f-909e-9591bfbad5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adc1790-0449-4339-a62c-7e2eb909019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182cca46-fd58-4e97-8f2a-d6dc1072fbfa",
   "metadata": {},
   "source": [
    "## Run Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2026e18f-54d2-40cc-b6d8-183574f9dafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d169866-3b02-47a3-ad85-edb4d48b4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state, pooled_output = output.last_hidden_state, output.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83589610-1a1b-4f89-9f49-fe4851603760",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c718a3-a49f-4452-ac07-e4060a0ca24f",
   "metadata": {},
   "source": [
    "## Feature Matrix Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d12d1-4b02-4b13-8c04-2f36f932e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313aee78-54f1-4937-aa27-1ced34ee6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = batch1.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e8f743-ceab-449c-9ee7-200a1785cb16",
   "metadata": {},
   "source": [
    "# Use BERT Representations with LogisticRegression Softmax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30dae3b5-f005-418c-8e46-9bd8758cf351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "import dataset\n",
    "import vsm\n",
    "import sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c995c70-1ec0-47c7-be85-5a622874c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER = 2\n",
    "TWITTER_AIRLINES = 3\n",
    "TWITTER_APPLE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5059a022-607d-493b-8e7e-82937409db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_train, twitter_validate, twitter_test =  dataset.dataset_reader(TWITTER)\n",
    "[twitter_train, twitter_validate, twitter_test] = list(map(lambda ds : dataset.prune_columns(2, ds), [twitter_train, twitter_validate, twitter_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b41be400-82cf-469e-96b0-522cf9de12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_weights_name = 'bert-base-cased'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_weights_name)\n",
    "bert_model = BertModel.from_pretrained(bert_weights_name)\n",
    "# model = BertForSequenceClassification.from_pretrained(bert_weights_name)\n",
    "# Unique values of sentiment\n",
    "twitter_sentiment_labels = twitter_train['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f23b9013-5def-4e2a-a699-3bb3b3cd984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_softmax_classifier(X, y):\n",
    "    mod = LogisticRegression(\n",
    "        fit_intercept=True,\n",
    "        solver='liblinear',\n",
    "        multi_class='ovr')\n",
    "    mod.fit(X, y)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "466a1797-1ce2-4a94-b900-776777db0d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_cls_phi(text):\n",
    "    # Get the ids. `vsm.hf_encode` will help; be sure to\n",
    "    # set `add_special_tokens=True`.\n",
    "    ##### YOUR CODE HERE\n",
    "    subtok_ids = vsm.hf_encode(text, bert_tokenizer, add_special_tokens=True)\n",
    "\n",
    "    # Get the BERT representations. `vsm.hf_represent` will help:\n",
    "    ##### YOUR CODE HERE\n",
    "    subtok_reps = vsm.hf_represent(subtok_ids, bert_model, layer=-1)\n",
    "\n",
    "    # Index into `reps` to get the representation above [CLS].\n",
    "    # The shape of `reps` should be (1, n, 768), where n is the\n",
    "    # number of tokens. You need the 0th element of the 2nd dim:\n",
    "    ##### YOUR CODE HERE\n",
    "    cls_rep = subtok_reps[0][:][0]\n",
    "\n",
    "    # These conversions should ensure that you can work with the\n",
    "    # representations flexibly. Feel free to change the variable\n",
    "    # name:\n",
    "    return cls_rep.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7bb900e-ec8f-4e7c-92e2-987c363777e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156831, 3000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_train.size, twitter_validate.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d2c69f6-6084-4316-899d-b2ec9d52d16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant      0.316     0.209     0.252       172\n",
      "    Negative      0.554     0.654     0.600       266\n",
      "     Neutral      0.522     0.460     0.489       285\n",
      "    Positive      0.536     0.621     0.575       277\n",
      "\n",
      "    accuracy                          0.513      1000\n",
      "   macro avg      0.482     0.486     0.479      1000\n",
      "weighted avg      0.499     0.513     0.502      1000\n",
      "\n",
      "CPU times: user 14min 46s, sys: 4.84 s, total: 14min 51s\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bert_experiment1500 = sst.experiment(\n",
    "    twitter_train[:1500], # \n",
    "    hf_cls_phi,\n",
    "    fit_softmax_classifier,\n",
    "    assess_dataframes=[twitter_validate[:1000]],\n",
    "    vectorize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a5cdd3f-d4b1-4ccd-9f65-1ddf47011ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant      0.405     0.285     0.334       172\n",
      "    Negative      0.548     0.662     0.600       266\n",
      "     Neutral      0.531     0.456     0.491       285\n",
      "    Positive      0.543     0.614     0.576       277\n",
      "\n",
      "    accuracy                          0.525      1000\n",
      "   macro avg      0.507     0.504     0.500      1000\n",
      "weighted avg      0.517     0.525     0.516      1000\n",
      "\n",
      "CPU times: user 23min 21s, sys: 8.04 s, total: 23min 29s\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bert_experiment3000 = sst.experiment(\n",
    "    twitter_train[:3000], # \n",
    "    hf_cls_phi,\n",
    "    fit_softmax_classifier,\n",
    "    assess_dataframes=[twitter_validate[:1000]],\n",
    "    vectorize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ec4292a-caa6-4f86-a98e-dd673983774d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant      0.443     0.273     0.338       172\n",
      "    Negative      0.571     0.714     0.634       266\n",
      "     Neutral      0.559     0.467     0.509       285\n",
      "    Positive      0.567     0.661     0.610       277\n",
      "\n",
      "    accuracy                          0.553      1000\n",
      "   macro avg      0.535     0.529     0.523      1000\n",
      "weighted avg      0.544     0.553     0.541      1000\n",
      "\n",
      "CPU times: user 40min 21s, sys: 13.5 s, total: 40min 34s\n",
      "Wall time: 6min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bert_experiment6000 = sst.experiment(\n",
    "    twitter_train[:6000], # \n",
    "    hf_cls_phi,\n",
    "    fit_softmax_classifier,\n",
    "    assess_dataframes=[twitter_validate[:1500]],\n",
    "    vectorize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbe91cba-a1d6-4bc5-8c0b-f2272bdee7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant      0.520     0.302     0.382       172\n",
      "    Negative      0.590     0.752     0.661       266\n",
      "     Neutral      0.571     0.519     0.544       285\n",
      "    Positive      0.596     0.650     0.622       277\n",
      "\n",
      "    accuracy                          0.580      1000\n",
      "   macro avg      0.569     0.556     0.552      1000\n",
      "weighted avg      0.574     0.580     0.569      1000\n",
      "\n",
      "CPU times: user 1h 14min 46s, sys: 25.6 s, total: 1h 15min 12s\n",
      "Wall time: 12min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bert_experiment12000 = sst.experiment(\n",
    "    twitter_train[:12000], # \n",
    "    hf_cls_phi,\n",
    "    fit_softmax_classifier,\n",
    "    assess_dataframes=[twitter_validate[:2000]],\n",
    "    vectorize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4326b274-a348-4623-a1eb-3142a2a52b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant      0.556     0.262     0.356       172\n",
      "    Negative      0.579     0.756     0.656       266\n",
      "     Neutral      0.599     0.565     0.581       285\n",
      "    Positive      0.607     0.664     0.634       277\n",
      "\n",
      "    accuracy                          0.591      1000\n",
      "   macro avg      0.585     0.562     0.557      1000\n",
      "weighted avg      0.588     0.591     0.577      1000\n",
      "\n",
      "CPU times: user 5h 6min 1s, sys: 1min 47s, total: 5h 7min 49s\n",
      "Wall time: 52min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bert_experiment_full = sst.experiment(\n",
    "    twitter_train, # \n",
    "    hf_cls_phi,\n",
    "    fit_softmax_classifier,\n",
    "    assess_dataframes=[twitter_validate],\n",
    "    vectorize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54b31f7-4cb2-4e55-b20d-ccde15e31350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
